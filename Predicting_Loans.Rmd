---
title: "Group Project 2"
author: "Knut Gjertsen"
date: "12/3/2017"
output: html_document
---


## Libraries and Data Setup 
```{r}
library(lubridate)
library(gmodels)
library(ggplot2)
library(dplyr)
library(class)
library(gmodels)
library(plotrix)

listings <- read.csv("Listings2013.csv")
```

## Data Cleaning
As part of our exploration of our dataset, we find several variables to be irrelevant to further analysis. The variables deleted are found irrelevant as they are aggregate variables or identical variables.
```{r}

sapply(data, function(x) sum(is.na(x)))



listings$number_of_days <- NULL
listings$principal_balance <- NULL
listings$loan_status_description <- NULL
listings$prosper_rating <- NULL
listings$listing_monthly_payment <- NULL

listings$loan_status[listings$loan_status == 1] <- 0
listings$loan_status[listings$loan_status == 4] <- 0

listings$loan_status[listings$loan_status == 2] <- 1
listings$loan_status[listings$loan_status == 3] <- 1

# remove NAs
listings <- listings[complete.cases(listings),]


```

For predictive purposes we will go ahead and assume that incomplete loans which are paid per schedule will be completed per schedule. Similarly, we will convert loans which are sold off to colleciton agencies as defaulted.
```{r}
listings$originaldate <- listings$loan_origination_date
listings$originaldate <- as.Date(listings$originaldate, "%m/%d/%Y")
listings$loan_origination_date <- as.Date(listings$loan_origination_date, "%m/%d/%Y")
listings$loan_origination_date <- as.numeric(listings$loan_origination_date)
listings$loan_origination_date <- listings$loan_origination_date-15705
```

##Time Analysis
We look to explore how the the different variables differ over time. 
###Analysis per Month
We will start by analysing the potential variation in the amount of loans taken and proportion of defaults among financial quarters.
```{r cars}
#We split up our time variable into the four financial quarters and create a new variable for this data to be saved to.
Calculatequarters <- function(date) {
  
  if(date <= 90) {
    return("1")
  } else if(date <= 182) {
    return("2")
  } else if(date <= 274) {
    return("3")
  } else {
     return("4")
  }
}
for(i in 1:NROW(listings$loan_origination_date)){
  listings$quarters[i] <- Calculatequarters(listings$loan_origination_date[i])
}
listings$quarters <- as.integer(listings$quarters)

ggplot(aes(x=quarters), data=listings) + geom_bar(aes(fill=factor(loan_status)))
```
This bar chart is highly interesting, as it seems as if people tend to increasingly take out loans as the year progresses.

Additionally, it seems as if the rate of default is lower as the year progresses, especially in the last financial quarter. To test this hypothesis we will calculate and plot the actual numbers for proportion of default.
```{r}
Q1 <- sum(listings$quarters == 1)
Q1_1 <- sum(listings$quarters == 1 & listings$loan_status==1)
Q1Ratio <- Q1_1/Q1
Q1Ratio

Q2 <- sum(listings$quarters == 2)
Q2_1 <- sum(listings$quarters == 2 & listings$loan_status==1)
Q2Ratio <- Q2_1/Q2
Q2Ratio

Q3 <- sum(listings$quarters == 3)
Q3_1 <- sum(listings$quarters == 3 & listings$loan_status==1)
Q3Ratio <- Q3_1/Q3
Q3Ratio

Q4 <- sum(listings$quarters == 4)
Q4_1 <- sum(listings$quarters == 4 & listings$loan_status==1)
Q4Ratio <- Q4_1/Q4
Q4Ratio

DefaultbyQuarter <- c(Q1Ratio, Q2Ratio, Q3Ratio, Q4Ratio)
plot(DefaultbyQuarter, type="o", col="blue")
```
Here it becomes entirely clear to us that the rate of default substantially decreases with each financial quarter. This is a valuable finding which calls for further exploration.

We will proceed by redoing the previous analysis per month. This could potentially show us whether certain months are drivers of the increased amount of loans and decreased default rates.
```{r}
Calculatemonths <- function(date2) {

  if(date2 <= 31) {
    return("1")
  } else if(date2 <= 59) {
    return("2")
  } else if(date2 <= 90) {
    return("3")
  } else if(date2 <= 120) {
    return("4")
  } else if(date2 <= 151) {
    return("5")
  } else if(date2 <= 181) {
    return("6")
  } else if(date2 <= 212) {
    return("7")
  } else if(date2 <= 243) {
    return("8")
  } else if(date2 <= 273) {
    return("9")
  } else if(date2 <= 304) {
    return("10")
  } else if(date2 <= 334) {
    return("11")
  } else {
     return("12")
  }
}

listings$months <- as.integer(listings$months)

for(i in 1:NROW(listings$loan_origination_date)){
  listings$months[i] <- Calculatemonths(listings$loan_origination_date[i])
}

ggplot(aes(x=months), data=listings) + geom_bar(aes(fill=factor(loan_status))) + scale_x_discrete(limits=c(1,2,3,4,5,6,7,8,9,10,11,12))
```
It appears as if there is near linear growth from January until September, which is quite interesting in itself. Additionally, for November and December there appears to be a non-proportional growth in default rates, which could be an option for arbitrage.

Let's continue by calculating the default rates.
```{r}
M1 <- sum(listings$months == 1)
M1_1 <- sum(listings$months == 1 & listings$loan_status==1)
M1Ratio <- M1_1/M1

M2 <- sum(listings$months == 2)
M2_1 <- sum(listings$months == 2 & listings$loan_status==1)
M2Ratio <- M2_1/M2

M3 <- sum(listings$months == 3)
M3_1 <- sum(listings$months == 3 & listings$loan_status==1)
M3Ratio <- M3_1/M3

M4 <- sum(listings$months == 4)
M4_1 <- sum(listings$months == 4 & listings$loan_status==1)
M4Ratio <- M4_1/M4

M5 <- sum(listings$months == 5)
M5_1 <- sum(listings$months == 5 & listings$loan_status==1)
M5Ratio <- M5_1/M5

M6 <- sum(listings$months == 6)
M6_1 <- sum(listings$months == 6 & listings$loan_status==1)
M6Ratio <- M6_1/M6

M7 <- sum(listings$months == 7)
M7_1 <- sum(listings$months == 7 & listings$loan_status==1)
M7Ratio <- M7_1/M7

M8 <- sum(listings$months == 8)
M8_1 <- sum(listings$months == 8 & listings$loan_status==1)
M8Ratio <- M8_1/M8

M9 <- sum(listings$months == 9)
M9_1 <- sum(listings$months == 9 & listings$loan_status==1)
M9Ratio <- M9_1/M9

M10 <- sum(listings$months == 10)
M10_1 <- sum(listings$months == 10 & listings$loan_status==1)
M10Ratio <- M10_1/M10

M11 <- sum(listings$months == 11)
M11_1 <- sum(listings$months == 11 & listings$loan_status==1)
M11Ratio <- M11_1/M11

M12 <- sum(listings$months == 12)
M12_1 <- sum(listings$months == 12 & listings$loan_status==1)
M12Ratio <- M12_1/M12

monthsratio <- c(M1Ratio,M2Ratio,M3Ratio,M4Ratio,M5Ratio,M6Ratio,M7Ratio,M8Ratio,M9Ratio,M10Ratio,M11Ratio,M12Ratio)
plot(monthsratio, type="o", col="blue")
```
As suspected, the default rates of especially November and December are substantially lower than other months.

However, to check whether an investor would be able to find opportunity for arbitrage here, one would need to measure up the rate of default to the interest rate of loans. It could be that the months of November and December are simply filled with ideal loan candidates, demanding low borrowing rates. Let's try to roughly estimate how the two match up over the year.
```{r}
#We create a range of months for our future data frame
monthsvector <- c(1,2,3,4,5,6,7,8,9,10,11,12)
#We use the tapply function to find the mean borrowing rate for each month.
monthsrate <- tapply(listings$borrower_rate, listings$months, mean)
#We recalculate the default ratio to a percentage to match the borrowing rate
monthsratio <- monthsratio*100
#Combine to data frame
dataframe <- data.frame(monthsvector, monthsratio, monthsrate)

#We plot the default rate up against the borrowing rate using a twoord.plot function. This enables us to look at the two line plots simultanously, but with two separate y-axes. 
twoord.plot(1:12, dataframe$monthsratio, 1:12, dataframe$monthsrate, xlab="Month",
            ylab="Default Rate",
            rylab="Borrower Rate",
            xtickpos = 1:12,
            xticklab=month.name[1:12])
```
It here becomes clear to us that there does not seem to be a significant enough drop in borrowing rate for November and December to divert from investing in November and December. In conclusion, we believe to have found significant arbitrage opportunities for future investing.

###Weekdays
As our previous analysis proved quite succesful, we will look towards further analysis how time might impact the default rate. This time we will further investigate the difference between weekdays and weekends.

We will start by labelling our date columns the name of each day.
```{r}
listings$day <- wday(listings$originaldate, label=TRUE)
summary(listings$day)
```
After splitting up and running a summation command we can see that the data set weirdly enough does not appear to include any Saturdays or Sundays. Upon re-inspecting the initial dataset, we could confirm that this was indeed the case.

We then proceed by comparing the different weekdays to each other. We start by investigating the difference in the rate of default.
```{r}
tapply(listings$loan_status, listings$day, mean)

```
The results show that there does not seem to be a significant difference in rate of default varying between weekdays.

Let's run it for a few other variables to see if we can find any interesting results.
```{r}
tapply(listings$borrower_rate, listings$day, mean)
tapply(listings$amount_funded, listings$day, mean)
tapply(listings$income_range, listings$day, mean)
tapply(listings$income_verifiable, listings$day, mean)
tapply(listings$monthly_debt, listings$day, mean)
```
Disappoingly, there does not seem to be any significant differences between weekdays for these variables. We will thus not conduct further investigatiom into the difference between weekdays.




Scorex is the credit score of the user, it goes from < 600, to 778+. 
```{r}
 
scorex <- listings %>%
    group_by(scorex) %>%
    summarise(count = n())

plot<-ggplot(data=scorex, aes(x=scorex, y=count, fill =scorex))+
  geom_bar(stat="identity" , position = "dodge")
plot


levels(listings$scorex) <- c(600,610,630,645,660,675,695,711,735.5,762, 778)

listings$scorex <- as.numeric(as.character(listings$scorex))

```
 
Prosper Score is a score from 1-11 with 1 being lowest risk. Because this is a score that we did not create we don't know what went into it. In addition the data they are using is probably the same data we are using so it does not provide any novel new information to the model. Lets remove it.
```{r}
listings$prosper_score <- NULL
```

listings category ID is a category of the Loan but there is no description to go along with the IDs. This information might be useful though. NNot sure what to do with this.
```{r}

```

Income range, Income Range Description, Stated Monthly Income. We don't want this datapoint to be overrepresented in the model. Therefore we only need one of these variables. Lets go with Stated Monthly Income since it is more granular than the broad income range. 
TO DO: Some of the algorithms will find that they are highly correlated and won't "Count them as double"... which ones?
```{r}
listings$income_range   <- NULL
listings$income_range_description  <- NULL

income <- listings %>%
    group_by(stated_monthly_income) %>%
    summarise(count = n())

str(listings$stated_monthly_income)
plot<-ggplot(data=listings, aes(listings$stated_monthly_income))+
  geom_histogram(binwidth = 250) + xlim(0, 50000)
plot
```

Income Verifiable, looks good and useful. Have to modify to be a factor first. 
```{r}

listings$income_verifiable <- factor(listings$income_verifiable)

verifiable <- listings %>%
    group_by(income_verifiable) %>%
    summarise(count = n())

plot<-ggplot(data=verifiable, aes(x=income_verifiable, y=count, fill=income_verifiable))+
  geom_bar(stat="identity" , position = "dodge")
plot

plot<-ggplot(data= listings, aes(x=income_verifiable, y=stated_monthly_income)) + geom_boxplot()  +   coord_cartesian(ylim=c(0, 15000))
plot
```

DTI WProsper Loan, debt to income ratio.
```{r}
loanRatioOne <- listings %>%
    filter(dti_wprosper_loan > 1) %>%
    summarise(count = n())
loanMax <- listings %>%
    filter(dti_wprosper_loan == 1000000) %>%
    summarise(count = n())
loanInBetween <- listings %>%
    filter(dti_wprosper_loan < 1000000  & dti_wprosper_loan > 1) %>%
    summarise(count = n())


listings$dti_wprosper_loan[listings$dti_wprosper_loan == 1000000.00] <- NA

listings$dti_wprosper_loan[is.na(listings$dti_wprosper_loan)] = median(listings$dti_wprosper_loan, na.rm=TRUE)


```
TO DO: Why does this have to be a value less than 1?


Employment Status Description & Employment.
There are blank values for occupation when someone chooses "Other for Employment Status Description". Lets fix this. 
```{r}


levels(listings$occupation)[levels(listings$occupation) == ""]<- "Other"

str(listings$employment_status_description)
levels(listings$employment_status_description)
levels(listings$employment_status_description)[levels(listings$employment_status_description) == "Not employed"] <- "Not-employed"


employment <- listings %>%
    group_by(employment_status_description) %>%
    summarise(count = n())

plot<-ggplot(data=employment, aes(x=employment_status_description, y=count, fill=employment_status_description))+
  geom_bar(stat="identity" , position = "dodge")
plot


listingssample <- listings[sample(nrow(listings)),]

listingssample$first_recorded_credit_line <- NULL   
#listingssample$borrower_city <- NULL   
#listingssample$borrower_state <- NULL  
listingssample$occupation <- NULL 
listingssample$public_records_last12_months <- NULL 
listingssample$lender_indicator <- NULL 


listingssample$is_homeowner <- as.factor(listingssample$is_homeowner)


listings$months_employed[listings$months_employed <0] <- NA

listings$months_employed[is.na(listings$months_employed)] = median(listings$months_employed, na.rm=TRUE)


```



##Feature Engineering and Geographic Exploration


```{r}

#converts  initials to full name
stateFromLower <-function(x) {
   #read 52 state codes into local variable [includes DC (Washington D.C. and PR (Puerto Rico)]
  st.codes<-data.frame(
                      state=as.factor(c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA",
                                         "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
                                         "MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM",
                                         "NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN",
                                         "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")),
                      full=as.factor(c("alaska","alabama","arkansas","arizona","california","colorado",
                                       "connecticut","district of columbia","delaware","florida","georgia",
                                       "hawaii","iowa","idaho","illinois","indiana","kansas","kentucky",
                                       "louisiana","massachusetts","maryland","maine","michigan","minnesota",
                                       "missouri","mississippi","montana","north carolina","north dakota",
                                       "nebraska","new hampshire","new jersey","new mexico","nevada",
                                       "new york","ohio","oklahoma","oregon","pennsylvania","puerto rico",
                                       "rhode island","south carolina","south dakota","tennessee","texas",
                                       "utah","virginia","vermont","washington","wisconsin",
                                       "west virginia","wyoming"))
                       )
     #create an nx1 data.frame of state codes from source column
  st.x<-data.frame(state=x)
     #match source codes with codes from 'st.codes' local variable and use to return the full state name
  refac.x<-st.codes$full[match(st.x$state,st.codes$state)]
     #return the full state names in the same order in which they appeared in the original source
  return(refac.x)
 
}
```


```{r}
#convert state name
listings$borrower_state <- trimws(listings$borrower_state)
listings$borrower_state <- as.factor(listings$borrower_state)
listings$region <- stateFromLower(listings$borrower_state)
```

```{r}
library(fiftystater)
data("fifty_states")
all_states <- map_data("state")
#plot all states with ggplot
keep <-c("region", "loan_status")
listingsByStates <- listings[keep]

listingsByStates<- listings %>%
     group_by(region) %>%
    summarise(sum = n(), prop = sum(loan_status == 1)/n())

  
states <-  distinct(all_states, region, .keep_all = TRUE)
map <- merge(states, listingsByStates, by="region", all.x=F)



p <- ggplot(listingsByStates, aes(map_id = region)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = prop), map = fifty_states) + 
  scale_fill_gradient(low = "yellow", high = "red") +
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank()) +
  ggtitle("States By Probability of Listings Default")

p + fifty_states_inset_boxes() 
```
A look at the probability of default by states.


## Cost of Living By State
```{r}

  col <- read.csv("costofliving.csv")
  col <- col[, 1:2]
  col$region <- tolower(col$State)
  listings<- merge(listings, col, by = "region")
  
  
  defaultByStateCOL <-listings %>%
      group_by(Rank) %>%
      summarise(sum = n(), prop = sum(loan_status == "Default")/n())
  
  plot<-ggplot(data=defaultByStateCOL, aes(x=Rank, y=prop, fill = (Rank)))+
    geom_bar(stat="identity" ) + geom_smooth()
  plot 

```


## Top 500 cities by GPD
```{r}

#cleaning
listings$borrower_city <- trimws(listings$borrower_city)
listings$borrower_city <- tolower(listings$borrower_city)
listings$borrower_city <- as.factor(listings$borrower_city)
citi <- read.csv("gdp_metro0917.csv")
citi$City <- tolower(citi$City)
citi$State <- NULL
library(tidyr)

citi <- citi%>%mutate(City=strsplit(as.character(City),'-'))%>%unnest(City)
citi<-citi[!(citi$City==""),]
listings$is_major_gdp <- ifelse(listings$borrower_city %in% citi$City, "Major", "Not Major")

defaultByGDP<- listings %>%
    group_by(is_major_gdp) %>%
    summarise(sum = n(), prop = sum(loan_status == 1)/n())

plot<-ggplot(data=defaultByGDP, aes(x=is_major_gdp, y=prop, fill = (is_major_gdp)))+
  geom_bar(stat="identity",  position = "dodge")
plot 
```
After separating the cities into two groups, either in the top 500 gpd producing metro's or not major, we find little correlation.

```{r}
#drop high factor level
#listingssample$borrower_city <- NULL   
#listingssample$borrower_state <- NULL
listingsample<- listings[, !(colnames(listings) %in% c("State", "region", "originaldate", "first_recorded_credit_line", "borrower_city", "borrower_state", "occupation", "public_records_last12_months"))]


```




## Data Models

# Data Prediction using Logistic Regression

#```{r}
logit.model <- glm(loan_status ~ ., data = listingssample, family = "binomial")
summary(logit.model)

logit.model1 <- glm(loan_status ~ amount_funded, borrower_rate, listing_term, scorex620-639, scorex640-649, scorex650-669, scorex665-689, scorex690-701, scorex702-723, scorex724-747, scorex748-777, scorex<600, stated_monthly_income, employment_status_descriptionOther, monthly_debt, inquiries_last6_months, bankcard_utilization, total_open_revolving_accounts, installment_balance, revolving_available_percent, total_inquiries, delinquencies_over30_days, delinquencies_over60_days, is_homeownerTRUE, data = listingssample, family = "binomial")

#Predicting the probability of an employee leaving using the Logit model
listings_logit <- NULL
listings_logit$loan_status <- predict(logit.model, newdata = listingssample, type = "response")

#Converting probabilities to a binary outcome (1 for Prob > 0.5, otherwise 0)
listings_logit$loan_status[listings_logit$loan_status > 0.5] <- 1
listings_logit$loan_status[listings_logit$loan_status != 1] <- 0

#Creating a cross table to verify the accuracy of the Logit model
#CrossTable(x = listings_logit$loan_status, y = listings[26834:33542, 11], prop.chisq = FALSE)
#```

# Getting Data Ready for Machine Learning Models


```{r}
levels(listingsample$employment_status_description) <- c("employed", "Fulltime", "NotEmployed", "other", "parttime", "retired", "selfEmployed" )
```

```{r}
# Randomizing  dataset
set.seed(12345)

listing_rand <- listingsample[order(runif(33300)),]
listing_rand$loan_status <- as.factor(listing_rand$loan_status)
listings_labels <- listing_rand[1]

l_dummy <-model.matrix(~ . -1, data = listing_rand[-1])
l_dummy <- as.data.frame(scale(l_dummy))
l_dummy$loan_status <- listings_labels$loan_status


#library(plyr)
#l_dummy<- rename(l_dummy, c("beta"="two", "gamma"="three"))
names(l_dummy)[names(l_dummy)=="scorex< 600"] <- "scorexLess"
names(l_dummy)[names(l_dummy)=="scorex600-619"] <- "sx619"
names(l_dummy)[names(l_dummy)=="scorex620-639"] <- "sx639"
names(l_dummy)[names(l_dummy)=="scorex640-649"] <- "sx649"
names(l_dummy)[names(l_dummy)=="scorex650-664"] <- "sx664"
names(l_dummy)[names(l_dummy)=="scorex665-689"] <- "sx689"
names(l_dummy)[names(l_dummy)=="scorex690-701"] <- "sx701"
names(l_dummy)[names(l_dummy)=="scorex702-723"] <- "sx723"
names(l_dummy)[names(l_dummy)=="scorex724-747"] <- "sx747"
names(l_dummy)[names(l_dummy)=="scorex748-777"] <- "sx777"
names(l_dummy)[names(l_dummy)=="scorex778+"] <- "sx778"

names(l_dummy)[names(l_dummy)=="day^4"] <- "day4"
names(l_dummy)[names(l_dummy)=="day^5"] <- "day5"
names(l_dummy)[names(l_dummy)=="day^6"] <- "day6"
names(l_dummy)[names(l_dummy)=="top_ten_gdpNot Major"] <- "nogdp"

train_data_indicies <- sample(1:nrow(l_dummy), 
                              replace = F, 
                              size = floor(nrow(l_dummy) * 0.75)) 
                  
train_data <- l_dummy[train_data_indicies, ]
test_data <- l_dummy[-train_data_indicies, ]
#listings_random <- listings[sample(nrow(listings)),]

round(prop.table(table(train_data$loan_status)) * 100, 1)

round(prop.table(table(test_data$loan_status)) * 100, 1)

```
# Rescaling listings dataset using Min-Max Normalization
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
  }
listings_n <- as.data.frame(lapply(listings_random[31:40], normalize))
str(listings_n)

# Subsetting listings dataset with 33542 enteries into test and train subsets, where 80% entries (26,833) are test and 20% entries (6,709) are train
listings_train <- listings_n[1:26833, 1:10]
listings_train_label <- listings_random[1:26833, 3]

listings_test <- listings_n[26834:33542, 1:10]
listings_test_label <- listings_random[26834:33542, 3]



listing_formula <- colnames(train_data) %>% 
    {paste(.[! . %in% "loan_status"], collapse = " + ")} %>% 
    paste("loan_status ~ ", .) %>% 
    as.formula()








# Data Prediction using kNN

```{r}
library(class)
library(gmodels)
knn_listing_pred <- knn(train = train_data, test = test_data,
                      cl = train_data$loan_status, k = 183) 
                      # k = sqrt(population size: 33542) = 183 
# confusion matrix
CrossTable(test_data$loan_status, knn_listing_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Loan Status', 'Predicted Loan Status'))


a2 = accuracy(knn_listing_pred, test_data$loan_status, "KNN ", TRUE)

```


#accuracy function

```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}
```


#Decision Tree

```{r}
library(C50)
default_decision_tree <- C5.0(train_data[-73], train_data$loan_status, trials = 3)

default_decision_tree

summary(default_decision_tree)

##Evaluating model performance ----
default_decision_pred <- predict(default_decision_tree, test_data)

# cross tabulation of predicted versus actual classes

CrossTable(test_data$loan_status, default_decision_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Loan Status', 'Predicted Loan Status'))

 
#Kappa(table(cancer_decision_pred, test_data$diagnosis))

a1= accuracy(default_decision_pred, test_data$loan_status, "C5.0 Decision Tree", TRUE)
```


random forest
```{r}
library(randomForest)
rf <- randomForest(loan_status ~ ., data = train_data)
rf_pred <- predict(rf, test_data)
a7 = accuracy(rf_pred, test_data$left, "Random Forest", TRUE)
```


