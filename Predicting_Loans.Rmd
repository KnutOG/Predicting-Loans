---
title: "Group Project 2"
author: "Knut Gjertsen"
date: "12/3/2017"
output: html_document
---


## Libraries and Data Setup 
```{r}
library(lubridate)
library(gmodels)
library(ggplot2)
library(dplyr)
library(class)
library(gmodels)
library(plotrix)

listings <- read.csv("Listings2013.csv")
```

## Data Cleaning
As part of our exploration of our dataset, we find several variables to be irrelevant to further analysis. The variables deleted are found irrelevant as they are aggregate variables or identical variables.
```{r}

sapply(data, function(x) sum(is.na(x)))



listings$number_of_days <- NULL
listings$principal_balance <- NULL
listings$loan_status_description <- NULL
listings$prosper_rating <- NULL
listings$listing_monthly_payment <- NULL

listings$loan_status[listings$loan_status == 1] <- 0
listings$loan_status[listings$loan_status == 4] <- 0

listings$loan_status[listings$loan_status == 2] <- 1
listings$loan_status[listings$loan_status == 3] <- 1

# remove NAs
listings <- listings[complete.cases(listings),]


```

For predictive purposes we will go ahead and assume that incomplete loans which are paid per schedule will be completed per schedule. Similarly, we will convert loans which are sold off to colleciton agencies as defaulted.
```{r}
listings$originaldate <- listings$loan_origination_date
listings$originaldate <- as.Date(listings$originaldate, "%m/%d/%Y")
listings$loan_origination_date <- as.Date(listings$loan_origination_date, "%m/%d/%Y")
listings$loan_origination_date <- as.numeric(listings$loan_origination_date)
listings$loan_origination_date <- listings$loan_origination_date-15705
```

##Time Analysis
We look to explore how the the different variables differ over time. 
###Analysis per Month
We will start by analysing the potential variation in the amount of loans taken and proportion of defaults among financial quarters.
```{r cars}
#We split up our time variable into the four financial quarters and create a new variable for this data to be saved to.
Calculatequarters <- function(date) {
  
  if(date <= 90) {
    return("1")
  } else if(date <= 182) {
    return("2")
  } else if(date <= 274) {
    return("3")
  } else {
     return("4")
  }
}
for(i in 1:NROW(listings$loan_origination_date)){
  listings$quarters[i] <- Calculatequarters(listings$loan_origination_date[i])
}
listings$quarters <- as.integer(listings$quarters)

ggplot(aes(x=quarters), data=listings) + geom_bar(aes(fill=factor(loan_status)))
```
This bar chart is highly interesting, as it seems as if people tend to increasingly take out loans as the year progresses.

Additionally, it seems as if the rate of default is lower as the year progresses, especially in the last financial quarter. To test this hypothesis we will calculate and plot the actual numbers for proportion of default.
```{r}
Q1 <- sum(listings$quarters == 1)
Q1_1 <- sum(listings$quarters == 1 & listings$loan_status==1)
Q1Ratio <- Q1_1/Q1
Q1Ratio

Q2 <- sum(listings$quarters == 2)
Q2_1 <- sum(listings$quarters == 2 & listings$loan_status==1)
Q2Ratio <- Q2_1/Q2
Q2Ratio

Q3 <- sum(listings$quarters == 3)
Q3_1 <- sum(listings$quarters == 3 & listings$loan_status==1)
Q3Ratio <- Q3_1/Q3
Q3Ratio

Q4 <- sum(listings$quarters == 4)
Q4_1 <- sum(listings$quarters == 4 & listings$loan_status==1)
Q4Ratio <- Q4_1/Q4
Q4Ratio

DefaultbyQuarter <- c(Q1Ratio, Q2Ratio, Q3Ratio, Q4Ratio)
plot(DefaultbyQuarter, type="o", col="blue")
```
Here it becomes entirely clear to us that the rate of default substantially decreases with each financial quarter. This is a valuable finding which calls for further exploration.

We will proceed by redoing the previous analysis per month. This could potentially show us whether certain months are drivers of the increased amount of loans and decreased default rates.
```{r}
Calculatemonths <- function(date2) {

  if(date2 <= 31) {
    return("1")
  } else if(date2 <= 59) {
    return("2")
  } else if(date2 <= 90) {
    return("3")
  } else if(date2 <= 120) {
    return("4")
  } else if(date2 <= 151) {
    return("5")
  } else if(date2 <= 181) {
    return("6")
  } else if(date2 <= 212) {
    return("7")
  } else if(date2 <= 243) {
    return("8")
  } else if(date2 <= 273) {
    return("9")
  } else if(date2 <= 304) {
    return("10")
  } else if(date2 <= 334) {
    return("11")
  } else {
     return("12")
  }
}

listings$months <- as.integer(listings$months)

for(i in 1:NROW(listings$loan_origination_date)){
  listings$months[i] <- Calculatemonths(listings$loan_origination_date[i])
}

ggplot(aes(x=months), data=listings) + geom_bar(aes(fill=factor(loan_status))) + scale_x_discrete(limits=c(1,2,3,4,5,6,7,8,9,10,11,12))
```
It appears as if there is near linear growth from January until September, which is quite interesting in itself. Additionally, for November and December there appears to be a non-proportional growth in default rates, which could be an option for arbitrage.

Let's continue by calculating the default rates.
```{r}
M1 <- sum(listings$months == 1)
M1_1 <- sum(listings$months == 1 & listings$loan_status==1)
M1Ratio <- M1_1/M1

M2 <- sum(listings$months == 2)
M2_1 <- sum(listings$months == 2 & listings$loan_status==1)
M2Ratio <- M2_1/M2

M3 <- sum(listings$months == 3)
M3_1 <- sum(listings$months == 3 & listings$loan_status==1)
M3Ratio <- M3_1/M3

M4 <- sum(listings$months == 4)
M4_1 <- sum(listings$months == 4 & listings$loan_status==1)
M4Ratio <- M4_1/M4

M5 <- sum(listings$months == 5)
M5_1 <- sum(listings$months == 5 & listings$loan_status==1)
M5Ratio <- M5_1/M5

M6 <- sum(listings$months == 6)
M6_1 <- sum(listings$months == 6 & listings$loan_status==1)
M6Ratio <- M6_1/M6

M7 <- sum(listings$months == 7)
M7_1 <- sum(listings$months == 7 & listings$loan_status==1)
M7Ratio <- M7_1/M7

M8 <- sum(listings$months == 8)
M8_1 <- sum(listings$months == 8 & listings$loan_status==1)
M8Ratio <- M8_1/M8

M9 <- sum(listings$months == 9)
M9_1 <- sum(listings$months == 9 & listings$loan_status==1)
M9Ratio <- M9_1/M9

M10 <- sum(listings$months == 10)
M10_1 <- sum(listings$months == 10 & listings$loan_status==1)
M10Ratio <- M10_1/M10

M11 <- sum(listings$months == 11)
M11_1 <- sum(listings$months == 11 & listings$loan_status==1)
M11Ratio <- M11_1/M11

M12 <- sum(listings$months == 12)
M12_1 <- sum(listings$months == 12 & listings$loan_status==1)
M12Ratio <- M12_1/M12

monthsratio <- c(M1Ratio,M2Ratio,M3Ratio,M4Ratio,M5Ratio,M6Ratio,M7Ratio,M8Ratio,M9Ratio,M10Ratio,M11Ratio,M12Ratio)
plot(monthsratio, type="o", col="blue")
```
As suspected, the default rates of especially November and December are substantially lower than other months.

However, to check whether an investor would be able to find opportunity for arbitrage here, one would need to measure up the rate of default to the interest rate of loans. It could be that the months of November and December are simply filled with ideal loan candidates, demanding low borrowing rates. Let's try to roughly estimate how the two match up over the year.
```{r}
#We create a range of months for our future data frame
monthsvector <- c(1,2,3,4,5,6,7,8,9,10,11,12)
#We use the tapply function to find the mean borrowing rate for each month.
monthsrate <- tapply(listings$borrower_rate, listings$months, mean)
#We recalculate the default ratio to a percentage to match the borrowing rate
monthsratio <- monthsratio*100
#Combine to data frame
dataframe <- data.frame(monthsvector, monthsratio, monthsrate)

#We plot the default rate up against the borrowing rate using a twoord.plot function. This enables us to look at the two line plots simultanously, but with two separate y-axes. 
twoord.plot(1:12, dataframe$monthsratio, 1:12, dataframe$monthsrate, xlab="Month",
            ylab="Default Rate",
            rylab="Borrower Rate",
            xtickpos = 1:12,
            xticklab=month.name[1:12])
```
It here becomes clear to us that there does not seem to be a significant enough drop in borrowing rate for November and December to divert from investing in November and December. In conclusion, we believe to have found significant arbitrage opportunities for future investing.

###Weekdays
As our previous analysis proved quite succesful, we will look towards further analysis how time might impact the default rate. This time we will further investigate the difference between weekdays and weekends.

We will start by labelling our date columns the name of each day.
```{r}
listings$day <- wday(listings$originaldate, label=TRUE)
summary(listings$day)
```
After splitting up and running a summation command we can see that the data set weirdly enough does not appear to include any Saturdays or Sundays. Upon re-inspecting the initial dataset, we could confirm that this was indeed the case.

We then proceed by comparing the different weekdays to each other. We start by investigating the difference in the rate of default.
```{r}
tapply(listings$loan_status, listings$day, mean)

```
The results show that there does not seem to be a significant difference in rate of default varying between weekdays.

Let's run it for a few other variables to see if we can find any interesting results.
```{r}
tapply(listings$borrower_rate, listings$day, mean)
tapply(listings$amount_funded, listings$day, mean)
tapply(listings$income_range, listings$day, mean)
tapply(listings$income_verifiable, listings$day, mean)
tapply(listings$monthly_debt, listings$day, mean)
```
Disappoingly, there does not seem to be any significant differences between weekdays for these variables. We will thus not conduct further investigatiom into the difference between weekdays.




Scorex is the credit score of the user, it goes from < 600, to 778+. 
```{r}
 
scorex <- listings %>%
    group_by(scorex) %>%
    summarise(count = n())

plot<-ggplot(data=scorex, aes(x=scorex, y=count, fill =scorex))+
  geom_bar(stat="identity" , position = "dodge")
plot


levels(listings$scorex) <- c(600,610,630,645,660,675,695,711,735.5,762, 778)

listings$scorex <- as.numeric(as.character(listings$scorex))

```
 
Prosper Score is a score from 1-11 with 1 being lowest risk. Because this is a score that we did not create we don't know what went into it. In addition the data they are using is probably the same data we are using so it does not provide any novel new information to the model. Lets remove it.
```{r}
listings$prosper_score <- NULL
```

listings category ID is a category of the Loan but there is no description to go along with the IDs. This information might be useful though. NNot sure what to do with this.
```{r}

```

Income range, Income Range Description, Stated Monthly Income. We don't want this datapoint to be overrepresented in the model. Therefore we only need one of these variables. Lets go with Stated Monthly Income since it is more granular than the broad income range. 
TO DO: Some of the algorithms will find that they are highly correlated and won't "Count them as double"... which ones?
```{r}
listings$income_range   <- NULL
listings$income_range_description  <- NULL

income <- listings %>%
    group_by(stated_monthly_income) %>%
    summarise(count = n())

str(listings$stated_monthly_income)
plot<-ggplot(data=listings, aes(listings$stated_monthly_income))+
  geom_histogram(binwidth = 250) + xlim(0, 50000)
plot
```

Income Verifiable, looks good and useful. Have to modify to be a factor first. 
```{r}

listings$income_verifiable <- factor(listings$income_verifiable)

verifiable <- listings %>%
    group_by(income_verifiable) %>%
    summarise(count = n())

plot<-ggplot(data=verifiable, aes(x=income_verifiable, y=count, fill=income_verifiable))+
  geom_bar(stat="identity" , position = "dodge")
plot

plot<-ggplot(data= listings, aes(x=income_verifiable, y=stated_monthly_income)) + geom_boxplot()  +   coord_cartesian(ylim=c(0, 15000))
plot
```

DTI WProsper Loan, debt to income ratio.
```{r}
loanRatioOne <- listings %>%
    filter(dti_wprosper_loan > 1) %>%
    summarise(count = n())
loanMax <- listings %>%
    filter(dti_wprosper_loan == 1000000) %>%
    summarise(count = n())
loanInBetween <- listings %>%
    filter(dti_wprosper_loan < 1000000  & dti_wprosper_loan > 1) %>%
    summarise(count = n())


listings$dti_wprosper_loan[listings$dti_wprosper_loan == 1000000.00] <- NA

listings$dti_wprosper_loan[is.na(listings$dti_wprosper_loan)] = median(listings$dti_wprosper_loan, na.rm=TRUE)


```
TO DO: Why does this have to be a value less than 1?


Employment Status Description & Employment.
There are blank values for occupation when someone chooses "Other for Employment Status Description". Lets fix this. 
```{r}


levels(listings$occupation)[levels(listings$occupation) == ""]<- "Other"

str(listings$employment_status_description)
levels(listings$employment_status_description)
levels(listings$employment_status_description)[levels(listings$employment_status_description) == "Not employed"] <- "Not-employed"


employment <- listings %>%
    group_by(employment_status_description) %>%
    summarise(count = n())

plot<-ggplot(data=employment, aes(x=employment_status_description, y=count, fill=employment_status_description))+
  geom_bar(stat="identity" , position = "dodge")
plot


#listingssample <- listings[sample(nrow(listings)),]




listingssample$is_homeowner <- as.factor(listingssample$is_homeowner)


listings$months_employed[listings$months_employed <0] <- NA

listings$months_employed[is.na(listings$months_employed)] = median(listings$months_employed, na.rm=TRUE)


```



##Feature Engineering and Geographic Exploration


```{r}

#converts  initials to full name
stateFromLower <-function(x) {
   #read 52 state codes into local variable [includes DC (Washington D.C. and PR (Puerto Rico)]
  st.codes<-data.frame(
                      state=as.factor(c("AK", "AL", "AR", "AZ", "CA", "CO", "CT", "DC", "DE", "FL", "GA",
                                         "HI", "IA", "ID", "IL", "IN", "KS", "KY", "LA", "MA", "MD", "ME",
                                         "MI", "MN", "MO", "MS",  "MT", "NC", "ND", "NE", "NH", "NJ", "NM",
                                         "NV", "NY", "OH", "OK", "OR", "PA", "PR", "RI", "SC", "SD", "TN",
                                         "TX", "UT", "VA", "VT", "WA", "WI", "WV", "WY")),
                      full=as.factor(c("alaska","alabama","arkansas","arizona","california","colorado",
                                       "connecticut","district of columbia","delaware","florida","georgia",
                                       "hawaii","iowa","idaho","illinois","indiana","kansas","kentucky",
                                       "louisiana","massachusetts","maryland","maine","michigan","minnesota",
                                       "missouri","mississippi","montana","north carolina","north dakota",
                                       "nebraska","new hampshire","new jersey","new mexico","nevada",
                                       "new york","ohio","oklahoma","oregon","pennsylvania","puerto rico",
                                       "rhode island","south carolina","south dakota","tennessee","texas",
                                       "utah","virginia","vermont","washington","wisconsin",
                                       "west virginia","wyoming"))
                       )
     #create an nx1 data.frame of state codes from source column
  st.x<-data.frame(state=x)
     #match source codes with codes from 'st.codes' local variable and use to return the full state name
  refac.x<-st.codes$full[match(st.x$state,st.codes$state)]
     #return the full state names in the same order in which they appeared in the original source
  return(refac.x)
 
}
```


```{r}
#convert state name
listings$borrower_state <- trimws(listings$borrower_state)
listings$borrower_state <- as.factor(listings$borrower_state)
listings$region <- stateFromLower(listings$borrower_state)
```

```{r}
library(fiftystater)
data("fifty_states")
all_states <- map_data("state")
#plot all states with ggplot
keep <-c("region", "loan_status")
listingsByStates <- listings[keep]

listingsByStates<- listings %>%
     group_by(region) %>%
    summarise(sum = n(), prop = sum(loan_status == 1)/n())

  
states <-  distinct(all_states, region, .keep_all = TRUE)
map <- merge(states, listingsByStates, by="region", all.x=F)



p <- ggplot(listingsByStates, aes(map_id = region)) + 
  # map points to the fifty_states shape data
  geom_map(aes(fill = prop), map = fifty_states) + 
  scale_fill_gradient(low = "yellow", high = "red") +
  expand_limits(x = fifty_states$long, y = fifty_states$lat) +
  coord_map() +
  scale_x_continuous(breaks = NULL) + 
  scale_y_continuous(breaks = NULL) +
  labs(x = "", y = "") +
  theme(legend.position = "bottom", 
        panel.background = element_blank()) +
  ggtitle("States By Probability of Listings Default")

p + fifty_states_inset_boxes() 
```
A look at the probability of default by states.


## Cost of Living By State
```{r}

  col <- read.csv("costofliving.csv")
  col <- col[, 1:2]
  col$region <- tolower(col$State)
  listings<- merge(listings, col, by = "region")
  
  
  defaultByStateCOL <-listings %>%
      group_by(Rank) %>%
      summarise(sum = n(), prop = sum(loan_status == "Default")/n())
  
  plot<-ggplot(data=defaultByStateCOL, aes(x=Rank, y=prop, fill = (Rank)))+
    geom_bar(stat="identity" ) + geom_smooth()
  plot 

```


## Top 500 cities by GPD
```{r}

#cleaning
listings$borrower_city <- trimws(listings$borrower_city)
listings$borrower_city <- tolower(listings$borrower_city)
listings$borrower_city <- as.factor(listings$borrower_city)
citi <- read.csv("gdp_metro0917.csv")
citi$City <- tolower(citi$City)
citi$State <- NULL
library(tidyr)

citi <- citi%>%mutate(City=strsplit(as.character(City),'-'))%>%unnest(City)
citi<-citi[!(citi$City==""),]
listings$is_major_gdp <- ifelse(listings$borrower_city %in% citi$City, "Major", "Not Major")
listings$is_major_gdp  <- as.factor(listings$is_major_gdp )
defaultByGDP<- listings %>%
    group_by(is_major_gdp) %>%
    summarise(sum = n(), prop = sum(loan_status == 1)/n())

plot<-ggplot(data=defaultByGDP, aes(x=is_major_gdp, y=prop, fill = (is_major_gdp)))+
  geom_bar(stat="identity",  position = "dodge")
plot 
```
After separating the cities into two groups, either in the top 500 gpd producing metro's or not major, we find little correlation.



Aggregate Variable
```{r}
listings$delinq_agg <- listings$delinquencies_over30_days + 2*listings$delinquencies_over60_days + 4*listings$delinquencies_over90_days
```


```{r}
#drop high factor level
#listingssample$borrower_city <- NULL   
#listingssample$borrower_state <- NULL

listingsample<- listings[, !(colnames(listings) %in% c("State", "region", "originaldate", "first_recorded_credit_line", "borrower_city", "borrower_state", "occupation", "public_records_last12_months", "Rank", "delinquencies_over30_days", "delinquencies_over90_days", "delinquencies_over60_days", "borrower_rate", "lender_indicator"))]
listingsample$months <- as.factor(listingsample$months)
```







## Data Models

# Data Prediction using Logistic Regression

```{r}
logReg <- glm(loan_status ~ ., data = listingsample, family = "binomial")
str(listingsample)
summary(logReg)
```


# Getting Data Ready for Machine Learning Models


```{r}
levels(listingsample$employment_status_description) <- c("employed", "Fulltime", "NotEmployed", "other", "parttime", "retired", "selfEmployed" )
```

```{r}
# Randomizing  dataset
set.seed(12345)

listing_rand <- listingsample[order(runif(33300)),]
listing_rand$loan_status <- as.factor(listing_rand$loan_status)
loan_status <- listing_rand[1]

l_dummy <-model.matrix(~ . -1, data = listing_rand[-1])
l_dummy <- as.data.frame(scale(l_dummy))
l_dummy<- cbind(loan_status, l_dummy)


#library(plyr)
#l_dummy<- rename(l_dummy, c("beta"="two", "gamma"="three"))
names(l_dummy)[names(l_dummy)=="scorex< 600"] <- "scorexLess"
names(l_dummy)[names(l_dummy)=="scorex600-619"] <- "sx619"
names(l_dummy)[names(l_dummy)=="scorex620-639"] <- "sx639"
names(l_dummy)[names(l_dummy)=="scorex640-649"] <- "sx649"
names(l_dummy)[names(l_dummy)=="scorex650-664"] <- "sx664"
names(l_dummy)[names(l_dummy)=="scorex665-689"] <- "sx689"
names(l_dummy)[names(l_dummy)=="scorex690-701"] <- "sx701"
names(l_dummy)[names(l_dummy)=="scorex702-723"] <- "sx723"
names(l_dummy)[names(l_dummy)=="scorex724-747"] <- "sx747"
names(l_dummy)[names(l_dummy)=="scorex748-777"] <- "sx777"
names(l_dummy)[names(l_dummy)=="scorex778+"] <- "sx778"

names(l_dummy)[names(l_dummy)=="day^4"] <- "day4"
names(l_dummy)[names(l_dummy)=="day^5"] <- "day5"
names(l_dummy)[names(l_dummy)=="day^6"] <- "day6"
names(l_dummy)[names(l_dummy)=="top_ten_gdpNot Major"] <- "nogdp"

train_data_indicies <- sample(1:nrow(l_dummy), 
                              replace = F, 
                              size = floor(nrow(l_dummy) * 0.8)) 
                  
train_data <- l_dummy[train_data_indicies, ]
test_data <- l_dummy[-train_data_indicies, ]
#listings_random <- listings[sample(nrow(listings)),]

round(prop.table(table(train_data$loan_status)) * 100, 1)

round(prop.table(table(test_data$loan_status)) * 100, 1)


#listing_formula <- colnames(train_data) %>% 
#    {paste(.[! . %in% "loan_status"], collapse = " + ")} %>% 
#    paste("loan_status ~ ", .) %>% 
#    as.formula()

```









#accuracy function

```{r}
accuracy <- function(predicted, trueval, model, hideoutput = F) {
  stopifnot(length(predicted) == length(trueval))
  result <- sum(predicted == trueval) / length(predicted)
  if (!hideoutput) {cat("Model:", model, "had", result, "accuracy\n")}
  return(result)
}
```


# Data Prediction using kNN

```{r}
library(class)
library(gmodels)
knn_listing_pred <- knn(train = train_data, test = test_data,
                      cl = train_data$loan_status, k = 183) 
                      # k = sqrt(population size: 33542) = 183 
# confusion matrix
CrossTable(test_data$loan_status, knn_listing_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Loan Status', 'Predicted Loan Status'))


a2 = accuracy(knn_listing_pred, test_data$loan_status, "KNN ", TRUE)

```




#Decision Tree

```{r}
library(C50)
default_decision_tree <- C5.0(train_data[-1], train_data$loan_status, trials = 3)

default_decision_tree

summary(default_decision_tree)

##Evaluating model performance ----
default_decision_pred <- predict(default_decision_tree, test_data)

# cross tabulation of predicted versus actual classes

CrossTable(test_data$loan_status, default_decision_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('Actual Loan Status', 'Predicted Loan Status'))

 
#Kappa(table(cancer_decision_pred, test_data$diagnosis))

a1= accuracy(default_decision_pred, test_data$loan_status, "C5.0 Decision Tree", TRUE)
```
### Improving Decision Trees: Boosting and Cost Matrix

Lets attempt to improve your decision tree model by adding Adaptive Boosting for 10 trials.

```{r}
# creating a decision tree prediction model using 10 trials
Decision_Tree_Model_Bagged <- C5.0(train_data[-1], train_data$loan_status, trials = 10)

# create a factor vector of predictions on test data
Decision_Tree_Predict_Bagged <- predict(Decision_Tree_Model_Bagged, train_data)

# creating a proportion table to check model accuacy
prop.table(table(Decision_Tree_Predict_Bagged == train_data$loan_status))

# cross tabulation of predicted v/s actual classes
CrossTable(test_data$loan_status, Decision_Tree_Predict_Bagged,
           dnn = c('actual loan_status', 'predicted loan_status'))
```

Lets attempt to improve your model with a cost matrix.

```{r}
# cost matrix
Decision_Tree_Error_Cost_Matrix <- matrix(c(0, 1, 4, 0), nrow = 2)

# create model - apply the cost matrix to the tree
Decision_Tree_Error_Cost_Model <- C5.0(train_data[-1], 
                                       train_data$loan_status,
                                       costs = Decision_Tree_Error_Cost_Matrix)

# predict - using model and test data set
Decision_Tree_Error_Cost_Prediction <- predict(Decision_Tree_Error_Cost_Model, test_data)


# creating a proportion table to check model accuacy
prop.table(table(Decision_Tree_Error_Cost_Prediction == train_data$loan_status))
# cross tabulation of predicted v/s actual classes
CrossTable(test_data$loan_status, Decision_Tree_Error_Cost_Prediction, 
           dnn = c('actual loan_status', 'predicted loan_status'))
```
### Cross Validation

run a 10-fold cross validation in an attempt to improve the model -

```{r}
# randomizing the data set
set.seed(12345)
folds <- createFolds(train_data$loan_status, k = 10)

folding_results <- lapply(folds, function(x) {
  fold_train <- train_data[x, ]
  fold_test <- test_data[-x, ]
  fold_model  <- C5.0(loan_status ~ ., data = fold_train)
  prediction <- predict(fold_model, fold_test)
  actual <- fold_test$loan_status
  kappa <- kappa2(data.frame(actual, prediction))$value
  return(kappa)
})

str(folding_results)
mean(unlist(folding_results))
```


# Data Prediction using Neural Nets

```{r}
library(neuralnet)
# confirm that the range is now between zero and one
train_data$loan_status <- as.numeric(train_data$loan_status)

# ANN w/ hidden neuron = 1
neuralNet_model <- neuralnet(formula = loan_status ~ scorex  + listing_category_id, data = train_data, hidden = 1)

# visualize the network topology
plot(neuralNet_model)

# compute predicted values using test data and neuralNet_model
neuralNet_results <- compute(neuralNet_model, train_data[-1])
predicted <- neuralNet_results$net.result

# examine the correlation between predicted and actual values
cor(predicted, test_data$loan_status)
```



